{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporte Practica 2: Procesamiento de datos <br />Carlos Espinoza - B92786"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>Limpia de Datos</strong>\n",
    "\n",
    "Es la identificación y corrección de errores dentro del dataset que pueden tener un impacto negativo en el modelo de predicción\n",
    "\n",
    "Dentro de los muchos errores que se pueden encontrar en un conjunto de datos  están las <strong>columnas que no aportan</strong> mucha información al modelado y las <strong>filas</strong> que se encuentran <strong>repetidas</strong>\n",
    "\n",
    "\n",
    "#### Identificación de columnas con un único valor\n",
    "- Estas columnas no aportan datos relevantes para el modelo de predicción\n",
    "<hr/>\n",
    "\n",
    "Código para detectar cuales son las columnas que deben ser removidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# summarize the number of unique values for each column using numpy\n",
    "from urllib.request import urlopen\n",
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "data = loadtxt(urlopen(path), delimiter=',')\n",
    "# summarize the number of unique values in each column\n",
    "for i in range(data.shape[1]):\n",
    "\tif len(unique(data[:,i])) == 1:\n",
    "\t\tprint(\"Se debe remover la columna\", i)\n",
    "\t#print(i, len(unique(data[:, i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de realizar esta tarea es utilizar la función de Pandas `nunique()` para poder obtener cuántos valores diferentes hay dentro de esa columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# summarize the number of unique values for each column using numpy\n",
    "from pandas import read_csv\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "# summarize the number of unique values in each column\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar la columnas que contienen un único valor\n",
    "\n",
    "Las columnas se pueden borrar fácilmente utilizando los módulos de numpy o Pandas, para ello existe la función `drop()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete columns with a single unique value\n",
    "from pandas import read_csv\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "print(df.shape)\n",
    "# get number of unique values for each column\n",
    "counts = df.nunique()\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if v == 1]\n",
    "print(to_del)\n",
    "# drop useless columns\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en el resultado de la impresión de las dimensiones, se eliminó una columna del dataset, específicamente la 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerar las columnas con pocos valores únicos\n",
    "\n",
    "Como se pudo observar antes, una columna con un único valor es de poca relevancias para el modelo de predicción, por esto era factible eliminarla. En el caso de tener más de un valor diferente, pero que igual siga siendo pequeño en relación con los valores alcanzados en otras columnas, es dificil saber si aportará al proceso de modelado o no.\n",
    "\n",
    "Dependiendo del algoritmo de predicción que se vaya a utilizar es probable que las columnas con pocos valores únicos generen problemas o resultados imprecisos.\n",
    "\n",
    "Podemos calcular el porcentaje de valores únicos en relación a la cantidad de filas que hay en el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the percentage of unique values for each column using numpy\n",
    "from urllib.request import urlopen\n",
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "data = loadtxt(urlopen(path), delimiter=',')\n",
    "# summarize the number of unique values in each column\n",
    "for i in range(data.shape[1]):\n",
    "\tnum = len(unique(data[:, i]))\n",
    "\tpercentage = float(num) / data.shape[0] * 100\n",
    "\tprint('%d, %d, %.1f%%' % (i, num, percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma podemos datos una idea de cuales podrían ser la columnas que nos pueden dar problemas con aquellos algoritmos que sean más sensibles a la varianza.\n",
    "\n",
    "Además, podemos hacer cambios en el código para ver cuales son aquellas columnas que se encuentran debajo del 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the percentage of unique values for each column using numpy\n",
    "from urllib.request import urlopen\n",
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "data = loadtxt(urlopen(path), delimiter=',')\n",
    "# summarize the number of unique values in each column\n",
    "for i in range(data.shape[1]):\n",
    "\tnum = len(unique(data[:, i]))\n",
    "\tpercentage = float(num) / data.shape[0] * 100\n",
    "\tif percentage < 1:\n",
    "\t\tprint('%d, %d, %.1f%%' % (i, num, percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resultado vemos que 11 de las 50 columnas dentro del conjunto de datos están por debajo del 1% en valores únicos. Ahora, esto únicamente nos dice que estos datos en específico requieren de un análisis más a fondo para determinar si es viable si borrado o no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remover las columnas que tienen poca varianza\n",
    "\n",
    "Otra forma de atacar el problema de la eliminación de columnas es ver cual es la varianza de la columna. Recordemos que la varianza es estática y puede ser utlizada como `filtro` para identificar columnas que deben ser eliminadas del conjunto de datos. \n",
    "\n",
    "Siguiendo con el ejemplo anterior, se puede ver que la columna 22 tiene una varianza de 0.0 pues esta solo tiene un dato único, mientras que las demás columnas con pocos valores únicos van a resultar con un grado de varianza cercano a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of apply the variance threshold\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "# split data into inputs and outputs\n",
    "data = df.values\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "print(X.shape, y.shape)\n",
    "# define the transform\n",
    "transform = VarianceThreshold()\n",
    "# transform the input data\n",
    "X_sel = transform.fit_transform(X)\n",
    "print(X_sel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the effect of the variance thresholds on the number of selected features\n",
    "from numpy import arange\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from matplotlib import pyplot\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "# split data into inputs and outputs\n",
    "data = df.values\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "print(X.shape, y.shape)\n",
    "# define thresholds to check\n",
    "thresholds = arange(0.0, 0.55, 0.05)\n",
    "# apply transform with each threshold\n",
    "results = list()\n",
    "for t in thresholds:\n",
    "\t# define the transform\n",
    "\ttransform = VarianceThreshold(threshold=t)\n",
    "\t# transform the input data\n",
    "\tX_sel = transform.fit_transform(X)\n",
    "\t# determine the number of input features\n",
    "\tn_features = X_sel.shape[1]\n",
    "\tprint('>Threshold=%.2f, Features=%d' % (t, n_features))\n",
    "\t# store the result\n",
    "\tresults.append(n_features)\n",
    "# plot the threshold vs the number of selected features\n",
    "pyplot.plot(thresholds, results)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este plot y con el resultado obtenido por consola se puede ver la cantidad de características o columnas que superan cierto grado de varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identificar filas que contengan datos duplicados\n",
    "\n",
    "Las filas que tienen datos iguales probablemente son poco útiles y hasta peligrosos pues llevan a una mala evaluación del modelo.\n",
    "\n",
    "Para detectar posibles duplicados dentro de un conjunto de datos, Pandas, tiene una función `duplicate()` para poder verificar si este es el caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate rows of duplicate data\n",
    "from pandas import read_csv\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "# calculate duplicates\n",
    "dups = df.duplicated()\n",
    "# report if there are any duplicates\n",
    "print(dups.any())\n",
    "# list all duplicate rows\n",
    "print(df[dups])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver en el resultado, Pandas se encargó de mostrar cuales fueron los datos duplicados. Veremos como eliminar estos datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar filas que contengan duplicados\n",
    "\n",
    "Probablemente todas las filas duplicadas deban ser eliminadas previo a un modelado. Para realizar esta tarea se utilizará la función de Pandas `drop_duplicates()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows of duplicate data from the dataset\n",
    "from pandas import read_csv\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "print(df.shape)\n",
    "# delete duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este resultado podemos ver el estado del conjunto de datos previo al borrado (150 filas) y después de este (147 filas), que serían las encontradas en la sección anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>Comentarios sobre el ejercicio</strong>\n",
    "\n",
    "Antes de esta práctica había trabajado muy poco con la parte de limpieza de datos, únicamente con las tareas más sencillas como lo son el borrado de duplicados. Me parece interesante esta nueva perspectiva que adquirí, de pensar que además de borrar filas, que suele ser lo más común, también se analice la utilidad de ciertas columnas o `features` dentro del dataset. También, me pareció interesante ver algunas de las técnicas utilizadas para determinar si una columna es de relevancia para el análisis, porque vimos que no por tener pocos valores únicos necesariamente se tiene que eliminar ese `feature`, en casos así se debe aplicar un análisis de varianzas para ayudar a tomar la decisión. \n",
    "\n",
    "Como mencioné anteriormente pocas veces he tenido que trabajar con estas técnicas pero siento que, aunque vimos pocas en el tutorial, son claves para llegar a tener un conjunto de datos mucho más limpio y además, son de gran ayuda para los analistas, pues nos dan una mejor visión del conjunto con el que estamos trabajando. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d84c614dc5133cfe8afdd35cd0e89d9e7c494d88a4d58a2a5280097bf69d7348"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
